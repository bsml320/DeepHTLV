{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ac1e539",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import h5py\n",
    "import scipy.io\n",
    "import random\n",
    "import sys,os\n",
    "import itertools\n",
    "import numbers\n",
    "from collections import Counter\n",
    "from warnings import warn\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66a5a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cf7fca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "np.random.seed(1337)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "#python_random.seed(1337)\n",
    "\n",
    "# The below set_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see:\n",
    "# https://www.tensorflow.org/api_docs/python/tf/random/set_seed\n",
    "#tf.random.set_seed(1337)\n",
    "#older version of tensorflow\n",
    "tf.set_random_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8784dce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baf00f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "# for gpu in gpus:\n",
    "#     tf.config.experimental.set_memory_growth(gpu, True)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"2,3,4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce833e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras.models import Sequential, model_from_yaml\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "import keras.layers.core as core\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Input, merge, multiply, Reshape\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.constraints import maxnorm\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Embedding\n",
    "from sklearn.metrics import fbeta_score, roc_curve, auc, roc_auc_score, average_precision_score, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.regularizers import l2, l1, l1_l2\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import activations, initializers, regularizers, constraints\n",
    "from keras.engine import InputSpec\n",
    "from keras.layers import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97ce723f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class Attention(Layer):\n",
    "\tdef __init__(self,hidden,init='glorot_uniform',activation='linear',W_regularizer=None,b_regularizer=None,W_constraint=None,**kwargs):\n",
    "\t\tself.init = initializers.get(init)\n",
    "\t\tself.activation = activations.get(activation)\n",
    "\t\tself.W_regularizer = regularizers.get(W_regularizer)\n",
    "\t\tself.b_regularizer = regularizers.get(b_regularizer)\n",
    "\t\tself.W_constraint = constraints.get(W_constraint)\n",
    "\t\tself.hidden=hidden\n",
    "\t\tsuper(Attention, self).__init__(**kwargs)\n",
    "\t    \n",
    "\tdef build(self, input_shape):\n",
    "\t\tinput_dim = input_shape[-1]\n",
    "\t\tself.input_length = input_shape[1]\n",
    "\t\tself.W0 = self.add_weight(name ='{}_W1'.format(self.name), shape = (input_dim, self.hidden), initializer = 'glorot_uniform', trainable=True) # Keras 2 API\n",
    "\t\tself.W  = self.add_weight( name ='{}_W'.format(self.name),  shape = (self.hidden, 1), initializer = 'glorot_uniform', trainable=True)\n",
    "\t\tself.b0 = K.zeros((self.hidden,), name='{}_b0'.format(self.name))\n",
    "\t\tself.b  = K.zeros((1,), name='{}_b'.format(self.name))\n",
    "\t\tself.trainable_weights = [self.W0,self.W,self.b,self.b0]\n",
    "\t    \n",
    "\t\tself.regularizers = []\n",
    "\t\tif self.W_regularizer:\n",
    "\t\t\tself.W_regularizer.set_param(self.W)\n",
    "\t\t\tself.regularizers.append(self.W_regularizer)\n",
    "\t\t\n",
    "\t\tif self.b_regularizer:\n",
    "\t\t\tself.b_regularizer.set_param(self.b)\n",
    "\t\t\tself.regularizers.append(self.b_regularizer)\n",
    "\t    \n",
    "\t\tself.constraints = {}\n",
    "\t\tif self.W_constraint:\n",
    "\t\t\tself.constraints[self.W0] = self.W_constraint\n",
    "\t\t\tself.constraints[self.W] = self.W_constraint\n",
    "\t\t\t\n",
    "\t\tsuper(Attention, self).build(input_shape)\n",
    "\t    \n",
    "\tdef call(self,x,mask=None):\n",
    "\t\tattmap = self.activation(K.dot(x, self.W0)+self.b0)\n",
    "\t\tattmap = K.dot(attmap, self.W) + self.b\n",
    "\t\tattmap = K.reshape(attmap, (-1, self.input_length)) # Softmax needs one dimension\n",
    "\t\tattmap = K.softmax(attmap)\n",
    "\t\tdense_representation = K.batch_dot(attmap, x, axes=(1, 1))\n",
    "\t\tout = K.concatenate([dense_representation, attmap]) # Output the attention maps but do not pass it to the next layer by DIY flatten layer\n",
    "\t\treturn out\n",
    "\n",
    "\n",
    "\tdef compute_output_shape(self, input_shape):\n",
    "\t    return (input_shape[0], input_shape[-1] + input_shape[1])\n",
    "\n",
    "\tdef get_config(self):\n",
    "\t\tconfig = {'init': 'glorot_uniform',\n",
    "\t\t\t\t\t'activation': self.activation.__name__,\n",
    "\t\t\t\t\t'W_constraint': self.W_constraint.get_config() if self.W_constraint else None,\n",
    "\t\t\t\t\t'W_regularizer': self.W_regularizer.get_config() if self.W_regularizer else None,\n",
    "\t\t\t\t\t'b_regularizer': self.b_regularizer.get_config() if self.b_regularizer else None,\n",
    "\t\t\t\t\t'hidden': self.hidden if self.hidden else None}\n",
    "\t\tbase_config = super(Attention, self).get_config()\n",
    "\t\treturn dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class attention_flatten(Layer): # Based on the source code of Keras flatten\n",
    "\tdef __init__(self, keep_dim, **kwargs):\n",
    "\t\tself.keep_dim = keep_dim\n",
    "\t\tsuper(attention_flatten, self).__init__(**kwargs)\n",
    "\n",
    "\tdef compute_output_shape(self, input_shape):\n",
    "\t\tif not all(input_shape[1:]):\n",
    "\t\t\traise Exception('The shape of the input to \"Flatten\" '\n",
    "\t\t\t\t\t\t\t'is not fully defined '\n",
    "\t\t\t\t\t\t\t'(got ' + str(input_shape[1:]) + '. '\n",
    "\t\t\t\t\t\t\t'Make sure to pass a complete \"input_shape\" '\n",
    "\t\t\t\t\t\t\t'or \"batch_input_shape\" argument to the first '\n",
    "\t\t\t\t\t\t\t'layer in your model.')\n",
    "\t\treturn (input_shape[0], self.keep_dim)   # Remove the attention map\n",
    "\n",
    "\tdef call(self, x, mask=None):\n",
    "\t\tx=x[:,:self.keep_dim]\n",
    "\t\treturn K.batch_flatten(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b27604b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "\tprint('building model')\n",
    "\n",
    "\tseq_input_shape = (1000,4)\n",
    "\tnb_filter = 256\n",
    "\tfilter_length = 9\n",
    "\tattentionhidden = 256\n",
    "\n",
    "\tseq_input = Input(shape = seq_input_shape, name = 'seq_input')\n",
    "\tconvul1   = Convolution1D(filters = nb_filter,\n",
    "                        \t  kernel_size = filter_length,\n",
    "                        \t  padding = 'valid',\n",
    "                        \t  activation = 'relu',\n",
    "                        \t  kernel_constraint = maxnorm(3),\n",
    "                        \t  subsample_length = 1)\n",
    "\n",
    "\tpool_ma1 = MaxPooling1D(pool_size = 3)\n",
    "\tdropout1 = Dropout(0.5977908689086315)\n",
    "\tdropout2 = Dropout(0.50131233477637737)\n",
    "\tdecoder  = Attention(hidden = attentionhidden, activation = 'linear')\n",
    "\tdense1   = Dense(1)\n",
    "\tdense2   = Dense(1)\n",
    "\n",
    "\toutput_1 = pool_ma1(convul1(seq_input))\n",
    "\toutput_2 = dropout1(output_1)\n",
    "\tatt_decoder  = decoder(output_2)\n",
    "\toutput_3 = attention_flatten(output_2._keras_shape[2])(att_decoder)\n",
    "\n",
    "\toutput_4 =  dense1(dropout2(Flatten()(output_2)))\n",
    "\tall_outp =  concatenate([output_3, output_4])\n",
    "\toutput_5 =  dense2(all_outp)\n",
    "\toutput_f =  Activation('sigmoid')(output_5)\n",
    "\n",
    "\tmodel = Model(inputs = seq_input, outputs = output_f)\n",
    "\tmodel.compile(loss = 'binary_crossentropy', optimizer = 'nadam', metrics = ['accuracy'])\n",
    "\n",
    "\tprint (model.summary())\n",
    "\treturn model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fff25ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model():\n",
    "\n",
    "    x_visdb = np.load('data/x_VISDB_fulldata.npy')\n",
    "    y_visdb = np.load('data/y_VISDB_fulldata.npy')\n",
    "\n",
    "    trainx, valx, trainy, valy = train_test_split(x_visdb, y_visdb, test_size = 0.1, random_state = 42)\n",
    "\n",
    "    model = build_model()\n",
    "    model.load_weights('model/Final_model.h5')\n",
    "\n",
    "    print('testing')\n",
    "    \n",
    "    y_pred = model.predict(valx, verbose = 1)\n",
    "\n",
    "    auroc = roc_auc_score(valy, y_pred)\n",
    "    aupr = average_precision_score(valy, y_pred)\n",
    "    \n",
    "    np.save('data/y_pred.npy', y_pred)\n",
    "    np.save('data/valy.npy', valy)\n",
    "\n",
    "    print('auroc = ', auroc)\n",
    "    print('aupr = ', aupr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d0cb864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model\n",
      "WARNING:tensorflow:From /Users/johnathanjia/opt/miniconda3/envs/deephtlv1/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /Users/johnathanjia/opt/miniconda3/envs/deephtlv1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:Large dropout rate: 0.597791 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "tracking <tf.Variable 'attention_1/attention_1_b0_1:0' shape=(256,) dtype=float32> b0\n",
      "tracking <tf.Variable 'attention_1/attention_1_b_1:0' shape=(1,) dtype=float32> b\n",
      "WARNING:tensorflow:Large dropout rate: 0.501312 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:From /Users/johnathanjia/opt/miniconda3/envs/deephtlv1/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "seq_input (InputLayer)          (None, 1000, 4)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 992, 256)     9472        seq_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 330, 256)     0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 330, 256)     0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 84480)        0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 586)          66049       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 84480)        0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_flatten_1 (attention_ (None, 256)          0           attention_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            84481       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 257)          0           attention_flatten_1[0][0]        \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            258         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1)            0           dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 160,260\n",
      "Trainable params: 160,260\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnathanjia/opt/miniconda3/envs/deephtlv1/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(filters=256, kernel_size=9, padding=\"valid\", activation=\"relu\", kernel_constraint=<keras.con..., strides=1)`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model\n",
      "WARNING:tensorflow:Large dropout rate: 0.597791 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "tracking <tf.Variable 'attention_2/attention_2_b0_1:0' shape=(256,) dtype=float32> b0\n",
      "tracking <tf.Variable 'attention_2/attention_2_b_1:0' shape=(1,) dtype=float32> b\n",
      "WARNING:tensorflow:Large dropout rate: 0.501312 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "seq_input (InputLayer)          (None, 1000, 4)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 992, 256)     9472        seq_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 330, 256)     0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 330, 256)     0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 84480)        0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 586)          66049       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 84480)        0           flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_flatten_2 (attention_ (None, 256)          0           attention_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            84481       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 257)          0           attention_flatten_2[0][0]        \n",
      "                                                                 dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            258         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1)            0           dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 160,260\n",
      "Trainable params: 160,260\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "testing\n",
      "WARNING:tensorflow:From /Users/johnathanjia/opt/miniconda3/envs/deephtlv1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "35066/35066 [==============================] - 270s 8ms/step\n",
      "auroc =  0.8537862099901068\n",
      "aupr =  0.37780924197665106\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\tbuild_model()\n",
    "\trun_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a93d68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
